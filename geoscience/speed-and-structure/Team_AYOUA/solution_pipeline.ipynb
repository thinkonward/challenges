{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db917dc",
   "metadata": {},
   "source": [
    "# Solution Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9579c",
   "metadata": {},
   "source": [
    "### Data Preprocessing / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ef879",
   "metadata": {},
   "source": [
    "We use the following data processing methods to improve the stability of training and the final performance: \n",
    "1) The input seismic data was normalized using the training data global statistics :\n",
    "    ```python\n",
    "   (mean = 0,std = 0.01550384)\n",
    "   ```\n",
    "3) Seismic data where resized using bilinear interpolation\n",
    "4) Some models use horizontal  flip data augmentation during training and tta2 test time augmentation during testing;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e42f81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model description\n",
    "\n",
    "The model predicts subsurface velocity maps from multi-component seismic shot gathers by combining a Vision Transformer (ViT) backbone with a learnable channel fusion step and a lightweight convolutional decoder.\n",
    "\n",
    "![model](assets/model.png)\n",
    "#### 1)  Input Preprocessing\n",
    "* Input shape: ```(Bs, C_in, T, R)```\n",
    "    * Bs: batch size\n",
    "    * C_in: number of input seismic components (5 shots )\n",
    "    * T: temporal samples\n",
    "    * R: receivers\n",
    "\n",
    "* Preprocessing: Each shot gather is resampled via bilinear interpolation to a fixed model input size (H_in, W_in) for patch-based processing.\n",
    "\n",
    "#### 2) Encoder – Vision Transformer\n",
    "* Base model: eva02 (base and large variants) from timm\n",
    "* Patch size: P × P\n",
    "* ````Number of tokens = num_patches_h*num_patches_w````, where:\n",
    "    ```python\n",
    "        num_patches_h = H_in // P\n",
    "        num_patches_w = W_in // P \n",
    "    ```    \n",
    "* Processing stages:\n",
    "    1. Local encoding: First split_at transformer blocks process each seismic component independently.\n",
    "    2. Channel Fusion : Two strategies are supported:\n",
    "        * Mean fusion: Simple average across the C_in components.\n",
    "        * Weighted fusion: Learnable softmax-normalized weights for each component.\n",
    "    3. Global encoding: After fusion, the remaining blocks integrate information across all components.\n",
    "#### 3) Decoder – Token-to-Velocity Map\n",
    "1. Projection: Tokens (excluding CLS) are projected from embed_dim to 512 channels.\n",
    "2. Reshape: Tokens are reshaped into a (num_patches_h, num_patches_w) spatial grid.\n",
    "3. Progressive upsampling using ConvTranspose2d layers:\n",
    "    ```python\n",
    "    (num_patches_h , num_patches_w)  → 2× → 4× → 8× → 16×\n",
    "    ```\n",
    "4. Final convolution: Produces a single-channel velocity map.\n",
    "5. Interpolation: Output is resized to (H_out, W_out) using bilinear interpolation.\n",
    "\n",
    "#### 4) Velocity Map Output\n",
    "1. Sigmoid activation: values in [0,1]\n",
    "2. Linear scaling:\n",
    "   ```python\n",
    "   velocity = 1.5 + 3.0 × sigmoid_output\n",
    "   ```\n",
    "### Training strategy\n",
    "* We employed a K-Fold cross-validation approach for model selection, followed by retraining on the entire dataset.\n",
    "* The model was optimized using a hybrid loss function that combines `L1Loss` and `Structural Similarity Index (SSIM)`\n",
    "* Exponential Moving Average (EMA) was applied to stabilize the training.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d252a",
   "metadata": {},
   "source": [
    "# Solution Reproduction Steps\n",
    "\n",
    "## 1. Pipeline Configuration\n",
    "\n",
    "Before running the pipeline, please make sure that all data are included in ```data``` folder. No other changes in the config file are required. \n",
    "\n",
    "CONFIGURATION CAVEAT:\n",
    "\n",
    "* If the evaluation instance has 4 GPU, feel free to add ```CUDA_VISIBLE_DEVICES=$num_gpus``` to the training script in (```exp/expXXX.sh```) files to run training in parallel;\n",
    "\n",
    "* Training was done on 2 RTX3090 gpu (24G of GPU RAM) with 96 CPU RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6a7fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Environment Setup\n",
    "Please, run the following command to install all needed libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542e3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01344be9",
   "metadata": {},
   "source": [
    "## 3. Data preparation step\n",
    "\n",
    "In this step, the training and test datasets are formatted to match the model pipeline.  \n",
    "Each training sample is stored as a single `.npy` file, where the five seismic source components are stacked along the channel dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from anytree import Node, RenderTree\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc9924",
   "metadata": {},
   "source": [
    "Befor running the below code, make sure to extract all the training data in ```data/train``` folder and the test data in ```data/test``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac20999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path\n",
    "training_dataset = \"./data/train/*\"  # 'path to your training data'\n",
    "test_dataset = \"./data/test/*\"  # 'path to your training data'\n",
    "out_train =\"./data/prep_train\"\n",
    "out_test =\"./data/prep_test\"\n",
    "os.makedirs(out_train,exist_ok=True)\n",
    "os.makedirs(out_test,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e312b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = glob(training_dataset)\n",
    "sample_ids = [path.split(\"/\")[-1] for path in sample_paths]\n",
    "source_coordinates = [1, 75, 150, 225, 300]\n",
    "\n",
    "for sample_path in tqdm(sample_paths,total=len(sample_paths)):\n",
    "    file_name=os.path.basename(sample_path)\n",
    "    \n",
    "    xs=[]\n",
    "    for i, s in enumerate(source_coordinates):\n",
    "        rec_data = np.load(os.path.join(sample_path, f\"receiver_data_src_{s}.npy\"))\n",
    "        xs.append(rec_data)\n",
    "    x_arr=np.stack(xs,0)\n",
    "    vel_data = np.load(os.path.join(sample_path, f\"vp_model.npy\"))\n",
    "    np.save(out_train+\"/\"+file_name+\"_input.npy\", x_arr)\n",
    "    np.save(out_train+\"/\"+file_name+\"_target.npy\", vel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dict(img_id=sample_ids))\n",
    "df.to_csv(\"data/meta_all.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = glob(test_dataset)\n",
    "sample_ids = [path.split(\"/\")[-1] for path in sample_paths]\n",
    "source_coordinates = [1, 75, 150, 225, 300]\n",
    "\n",
    "for sample_path in tqdm(sample_paths,total=len(sample_paths)):\n",
    "    file_name=os.path.basename(sample_path)\n",
    "    \n",
    "    xs=[]\n",
    "    for i, s in enumerate(source_coordinates):\n",
    "        rec_data = np.load(os.path.join(sample_path, f\"receiver_data_src_{s}.npy\"))\n",
    "        xs.append(rec_data)\n",
    "    x_arr=np.stack(xs,0)\n",
    "    \n",
    "    np.save(out_test+\"/\"+file_name+\".npy\", x_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c4d9c",
   "metadata": {},
   "source": [
    "## 4. Training step\n",
    "\n",
    "\n",
    "You can skip this step if you want to start with the pretrained checkpoints provided in ```./checkpoints```.\n",
    "Otherwise, uncomment and run the following command to trigger the model training script. \n",
    "\n",
    "The checkpoints will be saved to ```./inference_models``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! sh exp/exp149all.sh\n",
    "# ! sh exp/exp181all.sh\n",
    "# ! sh exp/exp187all.sh\n",
    "# ! sh exp/exp188all.sh\n",
    "# ! sh exp/exp189all.sh\n",
    "# ! sh exp/exp199all.sh\n",
    "# ! sh exp/exp207all.sh\n",
    "# ! sh exp/exp215all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665d622",
   "metadata": {},
   "source": [
    "## 5. Inference step\n",
    "Follow the steps below to run the model inference and generate predictions for the holdout dataset.  \n",
    "On the public test set, inference takes approximately 30 minutes when using a single RTX 3090 GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b48b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "from src.datasets.fwi import FWIDataset\n",
    "import numpy as np\n",
    "import json\n",
    "from run.init.model import init_model_from_config\n",
    "from run.init.forwarder import Forwarder\n",
    "from typing import Optional\n",
    "import cv2\n",
    "import yaml\n",
    "from glob import glob\n",
    "from torch import nn\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde4c78",
   "metadata": {},
   "source": [
    "In order to change model checkpoints used for inference adjust the ```model_dir_f``` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f557d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_names = [(f\"exp215_all{seed}\",True) for seed in [7,100,700,1000,70000]]\n",
    "exp_names += [(f\"exp207_all{seed}\",True) for seed in [7,100,700,1000,70000]]\n",
    "exp_names += [(f\"exp199_all{seed}\",False) for seed in [7,100,700,1000,70000]]\n",
    "exp_names += [(f\"exp189_all{seed}\",False) for seed in [70,100,700,1000,70000]]\n",
    "exp_names += [(f\"exp188_all{seed}\",False) for seed in [71,107, 7001, 10707,71777]]\n",
    "exp_names += [(f\"exp187_all{seed}\",False) for seed in [7,100,700,1000,70000]]\n",
    "exp_names += [(f\"exp181_all{seed}\",False) for seed in [7,100,700,1000,70000]]\n",
    "exp_names += [(f\"exp149_all{seed}\",False) for seed in [7,100,700,1000 ,70000]]\n",
    "model_dir_f=\"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb44d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer(model,  loader,tta=False):\n",
    "    predictions = []\n",
    "    img_ids = []\n",
    "    for batch in tqdm(loader):\n",
    "        img_ids.extend(batch[\"image_id\"])\n",
    "        batch[\"image\"] = batch[\"image\"].cuda()\n",
    "        with torch.no_grad():\n",
    "            if tta:\n",
    "                preds = model.predict_tta(batch) \n",
    "            else:\n",
    "                preds = model.predict(batch)\n",
    "            predictions.append(preds.cpu())\n",
    "                \n",
    "    return torch.concat(predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34fe33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_id = 'cuda:0' \n",
    "predictions=[]\n",
    "for i, (expname, tta) in enumerate(exp_names):\n",
    "    cfg = OmegaConf.load(f\"{model_dir_f}/{expname}/config.yaml\")\n",
    "    cfg.out_dir = \"predictions\"\n",
    "    test_df = FWIDataset.create_dataframe(\n",
    "            -1,\n",
    "            data_path=\"data/prep_test\",\n",
    "        ).sort_values(by=\"img_id\", key=lambda x: x)\n",
    "    dataset = FWIDataset(test_df, phase=\"test\", cfg=cfg.dataset)\n",
    "    inference_dtype = torch.float32\n",
    "    batch_size = 8\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    device = torch.device(device_id)\n",
    "    base_model= init_model_from_config(cfg.model)\n",
    "    model = Forwarder(cfg.forwarder, base_model).eval().to(device,dtype=inference_dtype)\n",
    "    \n",
    "    model.model.load_state_dict(\n",
    "        torch.load(f\"{model_dir_f}/{expname}/model_weights_ema.pth\", map_location=device),\n",
    "        strict=True\n",
    "    )\n",
    "    preds = infer(model,  loader,tta)\n",
    "    predictions.append(preds)\n",
    "final_preds = torch.median(torch.concat(predictions, dim=1), dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c332b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = final_preds.numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6054890",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = []\n",
    "for batch in tqdm(loader):\n",
    "    img_ids.extend(batch[\"image_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58dca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id=\"final_submission\"\n",
    "out_folder=f\"{sub_id}.npz\"\n",
    "\n",
    "for i,(sample_id, prediction) in enumerate(zip(img_ids, final_preds)):\n",
    "    create_submission(\n",
    "        sample_id, prediction, out_folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8de1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dab5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
