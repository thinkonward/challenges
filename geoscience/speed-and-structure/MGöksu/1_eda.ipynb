{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76a7ef5",
   "metadata": {},
   "source": [
    "NOTE: This notebook assumes that you have downloaded the competition data and saved it in `./data/speed-and-structure-train-data` and `./data/speed-and-structure-train-data-extended` directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36e88a",
   "metadata": {},
   "source": [
    "# Speed and Structure Competition\n",
    "\n",
    "## Part 1: EDA, K-Fold Preparation, and Simple Statistics\n",
    "\n",
    "---\n",
    "\n",
    "This is hardly an EDA. We are just preparing the dataset for the k-fold cross validation and collecting simple statistics to later use for data normalization during training.\n",
    "We are creating two k-fold files, one for the first dataset and one for the extended dataset, and then combining them into one file. This is not ideal but the extended dataset came out later and I already had a k-fold file for the first dataset that I use for preliminary experiments so I kept that for a fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052517cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"./data/speed-and-structure-train-data\"\n",
    "TRAIN_DIR_EXTENDED = \"./data/speed-and-structure-train-data-extended\"\n",
    "RECEIVER_IDS = [1, 75, 150, 225, 300]\n",
    "FOLDS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb30752",
   "metadata": {},
   "source": [
    "### K-Fold Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d4ea7",
   "metadata": {},
   "source": [
    "Get the dataset file list for the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cafe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_dirs = os.listdir(TRAIN_DIR)\n",
    "train_vel_files = []\n",
    "train_receiver_files = []\n",
    "for dir_id in train_sample_dirs:\n",
    "    train_vel_files.append(os.path.join(TRAIN_DIR, dir_id, \"vp_model.npy\"))\n",
    "    dir_id_receivers = []\n",
    "    for rec_id in RECEIVER_IDS:\n",
    "        dir_id_receivers.append(os.path.join(TRAIN_DIR, dir_id, f\"receiver_data_src_{rec_id}.npy\"))\n",
    "    train_receiver_files.append(dir_id_receivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f70e41",
   "metadata": {},
   "source": [
    "Uniformly assign a fold to each sample and save the fold information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a75669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for fold information 80% train, 20% test\n",
    "fold_info = pd.DataFrame()\n",
    "fold_info['dir_id'] = train_sample_dirs\n",
    "# assign a random fold to each sample uniformly\n",
    "folds = [i % FOLDS for i in range(len(train_sample_dirs))]\n",
    "random.seed(42)\n",
    "random.shuffle(folds)\n",
    "fold_info['fold'] = folds\n",
    "fold_info['vel_file'] = train_vel_files\n",
    "for i, rec_id in enumerate(RECEIVER_IDS):\n",
    "    fold_info[f'rec_{rec_id}'] = [rec_files[i] for rec_files in train_receiver_files]\n",
    "\n",
    "print(fold_info['fold'].value_counts())\n",
    "fold_info.to_csv(f'fold_info.csv', index=False)\n",
    "\n",
    "fold_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf14fa",
   "metadata": {},
   "source": [
    "Same operations for the extended data. We have pretty much a duplicate of the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f0da0",
   "metadata": {},
   "source": [
    "Getting the dataset file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31657d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_dirs = os.listdir(TRAIN_DIR_EXTENDED)\n",
    "train_vel_files = []\n",
    "train_receiver_files = []\n",
    "for dir_id in train_sample_dirs:\n",
    "    train_vel_files.append(os.path.join(TRAIN_DIR_EXTENDED, dir_id, \"vp_model.npy\"))\n",
    "    dir_id_receivers = []\n",
    "    for rec_id in RECEIVER_IDS:\n",
    "        dir_id_receivers.append(os.path.join(TRAIN_DIR_EXTENDED, dir_id, f\"receiver_data_src_{rec_id}.npy\"))\n",
    "    train_receiver_files.append(dir_id_receivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b132d",
   "metadata": {},
   "source": [
    "Uniformly assign a fold to each sample and save the fold information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf350774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for fold information 80% train, 20% test\n",
    "fold_info_extended = pd.DataFrame()\n",
    "fold_info_extended['dir_id'] = train_sample_dirs\n",
    "# assign a random fold to each sample uniformly\n",
    "folds = [i % FOLDS for i in range(len(train_sample_dirs))]\n",
    "random.seed(42)\n",
    "random.shuffle(folds)\n",
    "fold_info_extended['fold'] = folds\n",
    "fold_info_extended['vel_file'] = train_vel_files\n",
    "for i, rec_id in enumerate(RECEIVER_IDS):\n",
    "    fold_info_extended[f'rec_{rec_id}'] = [rec_files[i] for rec_files in train_receiver_files]\n",
    "\n",
    "print(fold_info_extended['fold'].value_counts())\n",
    "fold_info_extended.to_csv(f'fold_info_extended.csv', index=False)\n",
    "\n",
    "fold_info_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bd539",
   "metadata": {},
   "source": [
    "We now have two k-fold files, one for the original data and one for the extended data. We can now combine them into one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_info_all = pd.concat([fold_info, fold_info_extended], ignore_index=True)\n",
    "print(fold_info_all['fold'].value_counts())\n",
    "fold_info_all.to_csv(f'fold_info_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41267e2d",
   "metadata": {},
   "source": [
    "### Collection of Simple Statistics\n",
    "We are calculating the stats based on 500 samples. You can always get more precise results with more samples. Overall it shouldn't matter too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d094f",
   "metadata": {},
   "source": [
    "#### Velocity\n",
    "We will use Min-Max normalization on the velocity data during training. It looks like we have strict min and max boundaries in the velocity data. <br>\n",
    "\n",
    "See [Min-Max normalization](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = 500 # \n",
    "vel_list = []\n",
    "for vel_file in tqdm(train_vel_files[:sample_n]):\n",
    "    vel_list.append(np.load(vel_file))\n",
    "\n",
    "vel_array = np.concatenate(vel_list, axis=0)\n",
    "print(vel_array.shape)\n",
    "\n",
    "print(f\"Mean: {vel_array.mean()}\")\n",
    "print(f\"Std: {vel_array.std()}\")\n",
    "print(f\"Min: {vel_array.min()}\")\n",
    "print(f\"Max: {vel_array.max()}\")\n",
    "print(f\"Median: {np.median(vel_array)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f4362",
   "metadata": {},
   "source": [
    "#### Receiver\n",
    "Unlike the velocity data, we'll be using the mean and the std we calculated here for the Z-score normalization on the receiver data. <br>\n",
    "See [Z-score normalization](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13cd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = 500\n",
    "receiver_list = []\n",
    "for receiver_files in tqdm(train_receiver_files[:sample_n]):\n",
    "    for receiver_file in receiver_files:\n",
    "        receiver_list.append(np.load(receiver_file))\n",
    "receiver_array = np.concatenate(receiver_list, axis=0)\n",
    "print(receiver_array.shape)\n",
    "\n",
    "print(f\"Mean: {receiver_array.mean()}\")\n",
    "print(f\"Std: {receiver_array.std()}\")\n",
    "print(f\"Min: {receiver_array.min()}\")\n",
    "print(f\"Max: {receiver_array.max()}\")\n",
    "print(f\"Median: {np.median(receiver_array)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
