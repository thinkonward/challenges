{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4481faf8",
   "metadata": {},
   "source": [
    "NOTE: This notebook assumes that you have downloaded the competition data and saved it in `./data/speed-and-structure-train-data` and `./data/speed-and-structure-train-data-extended` directories. The 1_eda.ipynb notebook also must be run before this notebook to generate the `fold_info_all.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44324a",
   "metadata": {},
   "source": [
    "# Speed and Structure Competition\n",
    "\n",
    "## Part 2: Synthetic Data Generation\n",
    "\n",
    "---\n",
    "\n",
    "We are generating new velocity samples based on the training data. Each new sample is a weighted average of two random original samples. The weight is randomly sampled from a uniform distribution.\n",
    "\n",
    "$$\n",
    "V_{new} = w V_1 + (1 - w) V_2\n",
    "$$\n",
    "where $w \\in [0, 1]$.\n",
    "\n",
    "This is very loosely inspired by the oversampling method [SMOTE](https://arxiv.org/abs/1106.1813) with k=1. \n",
    "\n",
    "For each generated velocity sample, we use the devito library to simulate the seismic data.\n",
    "\n",
    "The generated data won't be exactly from the same distribution as the training data so we won't add any of them to the validation set. They will only be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63790fc9",
   "metadata": {},
   "source": [
    "Note that this notebook will generate 10 samples for demonstration purposes. You should generate more data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from examples.seismic import TimeAxis, RickerSource, Receiver, Model \n",
    "from devito import TimeFunction, Eq, solve, Operator\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110ba49",
   "metadata": {},
   "source": [
    "Important Note: I generated 20000 samples for the competition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I generated 20000 samples for the competition. \n",
    "GEN_N = 10 # This is for demonstration. You should generate more data for training.\n",
    "# GEN_N = 20000\n",
    "SRC_INDICES = [1, 75, 150, 225, 300]\n",
    "OUTPUT_DIR = \"./data/synth_data\"\n",
    "OUTPUT_FOLD_INFO_DIR = \"./\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TRAINING_DATASET_1 = \"./data/speed-and-structure-train-data-extended/*\"  \n",
    "TRAINING_DATASET_2 = \"./data/speed-and-structure-train-data/*\"  \n",
    "FOLD_INFO_FILE = \"./fold_info_all.csv\" # fold info for the original and extended dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ab1e9",
   "metadata": {},
   "source": [
    "### Simulation Configurations\n",
    "This class and the `vel_to_seis` function below is the best set of simulation configurations I found for the given dataset. I followed this thread in the competition forum [here](https://forum.thinkonward.com/t/synthatic-data-generation/2388/4) and the devito documentation [here](https://www.devitoproject.org/examples/seismic/tutorials/01_modelling.html). I wasn't able to run the simulation on a GPU so my implementation here is CPU based and probably suboptimal.\n",
    "\n",
    "If you don't explicitly set `dt=0.1`, the critical dt will be ~0.1548 and the resulting number of time steps will be 6461 instead of 10001. The generation process will be ~1.5 times faster but the data will have less resolution and obviously a different shape. For my solution, I don't mind the loss of fidelity since I'm already interpolating the data down to 2048 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelocityModel:\n",
    "    def __init__(self, t0, tn, \n",
    "                 v, \n",
    "                 src_ind,\n",
    "                 f0, rec_num, \n",
    "                 origin=(0., 0.), spacing=(1., 1.)):\n",
    "        self.t0 = t0\n",
    "        self.tn = tn\n",
    "        self.origin = origin\n",
    "        self.spacing = spacing\n",
    "\n",
    "        self.model = Model(\n",
    "                vp=v, \n",
    "                origin=self.origin, \n",
    "                shape=v.shape, \n",
    "                spacing=self.spacing,\n",
    "                space_order=2, \n",
    "                nbl=350, \n",
    "                bcs=\"damp\", \n",
    "                # dt=self.dt, # it will be ~0.1548 if we don't set dt\n",
    "            # dtype=np.float64\n",
    "        )\n",
    "        self.dt = self.model.critical_dt\n",
    "\n",
    "\n",
    "        self.time_range = TimeAxis(start=t0, stop=tn, step=self.dt)\n",
    "        # self.grid = Grid(shape=grid_shape, extent=grid_extent)\n",
    "        self.src = RickerSource(name='src', grid=self.model.grid, f0=f0, time_range=self.time_range)\n",
    "        self.src.coordinates.data[0, 0] = src_ind - 1\n",
    "        self.src.coordinates.data[0, -1] = 20.  # Depth is 20m\n",
    "\n",
    "        self.rec = Receiver(name='rec', grid=self.model.grid, time_range=self.time_range, npoint=rec_num)\n",
    "        \n",
    "        # this should be parametric\n",
    "        self.rec.coordinates.data[:, 0] = np.asarray([i for i in range(0, 301, 10)])\n",
    "        self.rec.coordinates.data[:, 1] = 10  # Depth is 10m\n",
    "\n",
    "\n",
    "        self.u = TimeFunction(name=\"u\", grid=self.model.grid, time_order=2, space_order=2)\n",
    "        self.pde = self.model.m * self.u.dt2 - self.u.laplace + self.model.damp * self.u.dt\n",
    "        self.stencil = Eq(self.u.forward, solve(self.pde, self.u.forward))\n",
    "\n",
    "        # Finally we define the source injection and receiver read function to generate the corresponding code\n",
    "        self.src_term = self.src.inject(field=self.u.forward, expr=self.src * self.dt**2 / self.model.m)\n",
    "\n",
    "        # Create interpolation expression for receivers\n",
    "        self.rec_term = self.rec.interpolate(expr=self.u.forward)\n",
    "\n",
    "        self.op = Operator([self.stencil] + self.src_term + self.rec_term, subs=self.model.spacing_map)\n",
    "\n",
    "    def get_seis(self):\n",
    "        self.op(time=self.time_range.num-1, dt=self.model.critical_dt)\n",
    "\n",
    "        rec_data_sim = np.asarray(self.rec.data)\n",
    "        # For some reason, removing the last row and padding the first row seems to help\n",
    "        # remove the last row\n",
    "        rec_data_sim = rec_data_sim[:-1]\n",
    "        # pad the first row\n",
    "        rec_data_sim = np.pad(rec_data_sim, ((1, 0), (0, 0)))\n",
    "\n",
    "        return rec_data_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1feaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vel_to_seis(target_data, src_ind):\n",
    "    vel_model = VelocityModel(\n",
    "        t0=0.,\n",
    "        tn=1000.,\n",
    "        v=target_data,\n",
    "        src_ind=src_ind,\n",
    "        f0=0.015,  # Source peak frequency is 15Hz (0.015 kHz)\n",
    "        rec_num=31\n",
    "    )\n",
    "\n",
    "    rec_data_sim = vel_model.get_seis()\n",
    "    return rec_data_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the names of subfolders (sample IDs)\n",
    "sample_paths_1 = glob(TRAINING_DATASET_1)\n",
    "sample_paths_2 = glob(TRAINING_DATASET_2)\n",
    "sample_paths = sample_paths_1 + sample_paths_2\n",
    "# extract the name of samples, i.e. sample IDs\n",
    "sample_ids = [path.split(\"/\")[-1] for path in sample_paths[:4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e12b5",
   "metadata": {},
   "source": [
    "### Synthetic Velocity Generation\n",
    "This is the function that returns a velocity model as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples:\", len(sample_paths))\n",
    "\n",
    "def create_new_vp(_sample_paths):\n",
    "    # pick two random samples\n",
    "    ind_1 = random.randint(0, len(_sample_paths) - 1)\n",
    "    ind_2 = random.randint(0, len(_sample_paths) - 1)\n",
    "\n",
    "    # target velocity model\n",
    "    target_data_1 = np.load(os.path.join(_sample_paths[ind_1], \"vp_model.npy\"))\n",
    "    target_data_2 = np.load(os.path.join(_sample_paths[ind_2], \"vp_model.npy\"))\n",
    "\n",
    "    # flip with prob 0.5\n",
    "    if random.random() > 0.5:\n",
    "        target_data_1 = np.flip(target_data_1, axis=0)\n",
    "    if random.random() > 0.5:\n",
    "        target_data_2 = np.flip(target_data_2, axis=0)\n",
    "\n",
    "    # weighted average\n",
    "    w = random.random()\n",
    "    target_data = w * target_data_1 + (1 - w) * target_data_2\n",
    "\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999fdfe5",
   "metadata": {},
   "source": [
    "### Synthetic Seismic Data Generation\n",
    "Here we use the `vel_to_seis` function to generate synthetic seismic data and compare it with the original seismic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0 # let's pick the first sample\n",
    "target_data = np.load(os.path.join(sample_paths[ind], \"vp_model.npy\"))\n",
    "src_ind = 150 # the source index (The third source)\n",
    "rec_data = np.load(os.path.join(sample_paths[ind], f\"receiver_data_src_{src_ind}.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_sim = vel_to_seis(target_data, src_ind)\n",
    "rec_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69f235",
   "metadata": {},
   "source": [
    "Since the original data has 10001 time steps and the generated data has 6461 time steps, we need to interpolate both to a smaller number of time steps (e.g. 1001) to make them comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate rec_sim from (6461, 31) to (1001, 31)\n",
    "original_rows = rec_sim.shape[0]\n",
    "target_rows = 1001\n",
    "x_original = np.linspace(0, 1, original_rows)\n",
    "x_new = np.linspace(0, 1, target_rows)\n",
    "\n",
    "# Interpolate along axis 0 (rows), keeping columns unchanged\n",
    "interpolator = interp1d(x_original, rec_sim, axis=0, kind='linear')\n",
    "rec_sim_interpolated = interpolator(x_new)\n",
    "\n",
    "print(rec_sim_interpolated.shape)  # Should print (1001, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9e5e9",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "We calculate how diffent they are on average and the maximum difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = rec_sim_interpolated - rec_data[::10, :] # downsample the original data\n",
    "print(np.abs(diff).mean())\n",
    "print(np.abs(diff).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27e9d5",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "We are making sure that they are visually similar as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(rec_sim_interpolated, aspect=\"auto\", cmap=\"jet\")\n",
    "ax[1].imshow(rec_data[::10, :], aspect=\"auto\", cmap=\"jet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7d174",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data\n",
    "Finally, we generate the synthetic data. We are making sure that they are not in the validation set by setting the fold to -1. New `dir_id`s are simply numbers starting from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef59214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for i in tqdm(range(GEN_N)):\n",
    "    dir_id = i+1\n",
    "    fold = -1 # set fold to -1 so that they will never be in any validation set\n",
    "    os.makedirs(f\"{OUTPUT_DIR}/{dir_id}\", exist_ok=True)\n",
    "\n",
    "    vel_file = f\"{OUTPUT_DIR}/{dir_id}/vp_model.npy\"\n",
    "    rec_1 = f\"{OUTPUT_DIR}/{dir_id}/receiver_data_src_1.npy\"\n",
    "    rec_75 = f\"{OUTPUT_DIR}/{dir_id}/receiver_data_src_75.npy\"\n",
    "    rec_150 = f\"{OUTPUT_DIR}/{dir_id}/receiver_data_src_150.npy\"\n",
    "    rec_225 = f\"{OUTPUT_DIR}/{dir_id}/receiver_data_src_225.npy\"\n",
    "    rec_300 = f\"{OUTPUT_DIR}/{dir_id}/receiver_data_src_300.npy\"\n",
    "\n",
    "    df_list.append({\"dir_id\": dir_id, \"fold\": fold, \"vel_file\": vel_file, \"rec_1\": rec_1, \"rec_75\": rec_75, \"rec_150\": rec_150, \"rec_225\": rec_225, \"rec_300\": rec_300})\n",
    "\n",
    "    # check if file exists\n",
    "    if os.path.exists(vel_file) and os.path.exists(rec_1) and os.path.exists(rec_75) and os.path.exists(rec_150) and os.path.exists(rec_225) and os.path.exists(rec_300):\n",
    "        print(\"File already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    new_target_data = create_new_vp(sample_paths)\n",
    "    rec_data_sim = []\n",
    "    for src_ind in SRC_INDICES:\n",
    "        rec_data_ind = vel_to_seis(new_target_data, src_ind)\n",
    "        rec_data_sim.append(rec_data_ind)\n",
    "    # This loop can be parallelized like so\n",
    "    # rec_data_sim = Parallel(n_jobs=4)(delayed(vel_to_seis)(new_target_data, src_ind) for src_ind in SRC_INDICES)\n",
    "    \n",
    "    # save the data\n",
    "    np.save(vel_file, new_target_data)\n",
    "    for i, s in enumerate(SRC_INDICES):\n",
    "        rec_data = rec_data_sim[i]\n",
    "        np.save(f\"{OUTPUT_DIR}/{dir_id}/receiver_data_src_{s}.npy\", rec_data)\n",
    "\n",
    "df = pd.DataFrame(df_list, columns=[\"dir_id\", \"fold\", \"vel_file\", \"rec_1\", \"rec_75\", \"rec_150\", \"rec_225\", \"rec_300\"])\n",
    "df.to_csv(f\"{OUTPUT_FOLD_INFO_DIR}/fold_info_synth_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd272813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new fold_info file including the synthetic data\n",
    "fold_info_orig = pd.read_csv(FOLD_INFO_FILE)\n",
    "fold_info_with_synth = pd.concat([fold_info_orig, df], ignore_index=True)\n",
    "fold_info_with_synth.to_csv(f\"{OUTPUT_FOLD_INFO_DIR}/fold_info_all_with_synth.csv\", index=False)\n",
    "fold_info_with_synth.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
