{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c224395f",
   "metadata": {},
   "source": [
    "NOTE: This notebook assumes that you have downloaded the competition data and saved it in `./data/speed-and-structure-train-data` and `./data/speed-and-structure-train-data-extended` directories. The 1_eda.ipynb and 2_data_gen.ipynb notebook also must be run before this notebook to generate the `fold_info_all_with_synth.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044eaf8",
   "metadata": {},
   "source": [
    "# Speed and Structure Competition\n",
    "\n",
    "## Part 3: Training\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs one fold of training based on the `cfg` from `config.py`. My final submission used 3 folds of training. I only changed the `cfg.holdout_idx` variable to 1, 2, and 3. Therefore, for reproduction, you must run this notebook 3 times. The resulting models are saved in the `./model_checkpoints` directory.\n",
    "\n",
    "---\n",
    "### Some notes for reproducibility\n",
    "- `config.py` file is same as my final submission. You just need to change the `cfg.holdout_idx` variable and rerun.\n",
    "- Originally, I used batch_size=2 and grad_accum=4 having an effective batch size of 8. Depending on your GPU RAM, you may want to change this. The model is not affected by batch statistics, so it should be theoretically equivalent.\n",
    "- Dataset class loads all the data. That is not much of a problem initially but if you want to generate a lot of new data, you may want to modify this class to load data lazily.\n",
    "- `cfg.target_len` is the number of input time steps to interpolate down to (e.g. 10001 -> 2048 for the original data or 6461 -> 2048 for the synthetic data). You can increase this value as much as your GPU can handle. I realize now that this naming is terrible, sorry about that :(\n",
    "- Although the training is seeded, there are randomness from data augmentation (horizontal flip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from config import cfg\n",
    "from model import Net, ModelEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7dea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./model_checkpoints\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce92686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd10de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mode='train', fold_info_file=None, cfg=None):\n",
    "        self.mode = mode\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.receiver_ids = [1, 75, 150, 225, 300]\n",
    "        self.fold_info_file = fold_info_file\n",
    "        self.holdout_idx = cfg.holdout_idx\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        fold_df = pd.read_csv(self.fold_info_file)\n",
    "        if self.mode == 'train':\n",
    "            vel_files = fold_df[fold_df['fold'] != self.holdout_idx]['vel_file'].tolist()\n",
    "            rec_files = fold_df[fold_df['fold'] != self.holdout_idx][[f\"rec_{rec_id}\" for rec_id in self.receiver_ids]].values.tolist()\n",
    "        else:\n",
    "            vel_files = fold_df[fold_df['fold'] == self.holdout_idx]['vel_file'].tolist()\n",
    "            rec_files = fold_df[fold_df['fold'] == self.holdout_idx][[f\"rec_{rec_id}\" for rec_id in self.receiver_ids]].values.tolist()\n",
    "\n",
    "        for vel_file in vel_files:\n",
    "            vel_np = np.load(vel_file)\n",
    "            self.labels.append(vel_np)\n",
    "\n",
    "        for dir_rec_files in rec_files:\n",
    "            recs = []\n",
    "            for dir_rec_file in dir_rec_files:\n",
    "                rec = np.load(dir_rec_file) # rec.shape = (10001, 31)\n",
    "                \n",
    "                if cfg.target_len is not None:\n",
    "                    original_rows = rec.shape[0]\n",
    "                    target_rows = cfg.target_len\n",
    "                    x_original = np.linspace(0, 1, original_rows)\n",
    "                    x_new = np.linspace(0, 1, target_rows)\n",
    "\n",
    "                    # Interpolate along axis 0 (rows), keeping columns unchanged\n",
    "                    interpolator = interp1d(x_original, rec, axis=0, kind='linear')\n",
    "                    rec = interpolator(x_new).astype(np.float32)\n",
    "                    rec = np.pad(rec, ((0, 0), (0, 32 - rec.shape[1])), mode='constant', constant_values=0)\n",
    "\n",
    "                recs.append(rec)\n",
    "            if cfg.one_channel:\n",
    "                self.data.append(np.expand_dims(np.concatenate(recs, axis=1), 0))\n",
    "            else:\n",
    "                self.data.append(np.stack(recs, axis=0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx].copy()\n",
    "        label = self.labels[idx].copy()\n",
    "\n",
    "        if self.mode == 'train' and self.cfg.horizontal_flip and np.random.random() < self.cfg.hflip_prob:\n",
    "            data = data[::-1, :, ::-1]\n",
    "            label = label[::-1, ...]\n",
    "\n",
    "        if self.cfg.x_norm:\n",
    "            data = (data - self.cfg.x_mean) / self.cfg.x_std\n",
    "\n",
    "        if self.cfg.y_norm:\n",
    "            label = (label - self.cfg.y_median) / self.cfg.y_std\n",
    "        \n",
    "        if self.cfg.y_min_max_norm:\n",
    "            label = (label - self.cfg.y_min) / (self.cfg.y_max - self.cfg.y_min)\n",
    "        \n",
    "        return data.copy(), label.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb91d4",
   "metadata": {},
   "source": [
    "### Training Loss\n",
    "Directly optimizing for MAPE loss seems to give slightly better results. We just need to check if there is any normalization and undo that before calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacf726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_loss(y_true, y_pred):\n",
    "    # de-normalize, then calculate the loss\n",
    "    if cfg.y_norm:\n",
    "        output = y_pred * cfg.y_std + cfg.y_median\n",
    "        label = y_true * cfg.y_std + cfg.y_median\n",
    "    if cfg.y_min_max_norm:\n",
    "        output = y_pred * (cfg.y_max - cfg.y_min) + cfg.y_min\n",
    "        label = y_true * (cfg.y_max - cfg.y_min) + cfg.y_min\n",
    "    return torch.mean(torch.abs((label - output) / (label)))\n",
    "criterion_metric = mape_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ae47d",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(cfg.seed)\n",
    "\n",
    "model = Net(backbone=cfg.backbone, \n",
    "            pretrained=cfg.backbone_pretrained, \n",
    "            fuse_ch=cfg.fuse_ch,\n",
    "            one_channel=cfg.one_channel,\n",
    "            norm_layer=cfg.norm_layer,\n",
    "            dropout=cfg.dropout,\n",
    "            y_min_max_norm=cfg.y_min_max_norm,\n",
    "            horizontal_tta=cfg.horizontal_tta\n",
    "            ).to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3696e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.loss == 'mape':\n",
    "    criterion = mape_loss\n",
    "elif cfg.loss == \"mae\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.load_model_path is not None:\n",
    "    model.load_state_dict(torch.load(cfg.load_model_path, \n",
    "                                        map_location=cfg.device\n",
    "                                        ))\n",
    "    print(f\"Loaded model from {cfg.load_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb39b0",
   "metadata": {},
   "source": [
    "### EMA model\n",
    "Exponential Moving Average model is a very simple and easy-to-plug technique. See [here](https://timm.fast.ai/training_modelEMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ed328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.ema:\n",
    "    print(\"Initializing EMA model..\")\n",
    "    ema_model = ModelEMA(\n",
    "        model, \n",
    "        decay=cfg.ema_decay, \n",
    "        device=cfg.device,\n",
    "    )\n",
    "else:\n",
    "    ema_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(mode='train', fold_info_file=cfg.fold_info_file, cfg=cfg)\n",
    "val_dataset = CustomDataset(mode='val', fold_info_file=cfg.fold_info_file, cfg=cfg)\n",
    "train_dl = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, \n",
    "                    num_workers=cfg.num_workers)\n",
    "val_dl = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, \n",
    "                    num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd796f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.optimizer == \"adam\":\n",
    "    optim = torch.optim.Adam(model.parameters(), \n",
    "                            lr=cfg.lr, \n",
    "                            weight_decay=cfg.weight_decay,\n",
    "                            )\n",
    "elif cfg.optimizer == \"adamw\":\n",
    "    optim = torch.optim.AdamW(model.parameters(), \n",
    "                            lr=cfg.lr, \n",
    "                            weight_decay=cfg.weight_decay,\n",
    "                            )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if cfg.scheduler == \"cosine\":\n",
    "    t_max = len(train_dl) * cfg.n_epochs\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=t_max, eta_min=cfg.eta_min)\n",
    "else:\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c499bfe4",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "print(f\"Started training on {cfg.device}\")\n",
    "for epoch in range(cfg.n_epochs + 1):\n",
    "    if epoch != 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_mape_losses = []\n",
    "        optim.zero_grad()\n",
    "\n",
    "        for batch_idx, (data, label) in enumerate(tqdm(train_dl, leave=False)):\n",
    "            data = data.to(cfg.device)\n",
    "            label = label.to(cfg.device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(label, output)\n",
    "            loss.backward()\n",
    "\n",
    "            if ( batch_idx + 1 ) % cfg.grad_accum == 0:\n",
    "                if cfg.grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "\n",
    "            cur_train_loss = loss.item()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            train_losses.append(cur_train_loss)\n",
    "\n",
    "            if ema_model is not None:\n",
    "                ema_model.update(model)\n",
    "\n",
    "            cur_mape_loss = criterion_metric(label, output)\n",
    "\n",
    "            train_mape_losses.append(cur_mape_loss.item())\n",
    "            if batch_idx % cfg.print_every == 0:\n",
    "                print(f\"Epoch {epoch+1}/{cfg.n_epochs+1}, Batch {batch_idx}/{len(train_dl)}, \")\n",
    "                print(f\"Loss: {np.mean(train_losses):.4f}, MAPE Loss: {np.mean(train_mape_losses):.4f}, LR: {scheduler.get_last_lr()[0]:.3e}\")\n",
    "                train_losses = []\n",
    "                train_mape_losses = []\n",
    "                \n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_mape_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(tqdm(val_dl, leave=False)):\n",
    "            data = data.to(cfg.device)\n",
    "            label = label.to(cfg.device)\n",
    "            if ema_model is not None:\n",
    "                output = ema_model.module(data)\n",
    "            else:\n",
    "                output = model(data)\n",
    "            loss = criterion(label, output)\n",
    "            cur_val_loss = loss.item()\n",
    "            val_losses.append(cur_val_loss)\n",
    "            cur_mape_loss = criterion_metric(label, output)\n",
    "\n",
    "            val_mape_losses.append(cur_mape_loss.item())\n",
    "    val_mape_loss = np.mean(val_mape_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch+1}/{cfg.n_epochs+1}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}/{cfg.n_epochs+1}, Val MAPE Loss: {val_mape_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f\"Saving best model\")\n",
    "        if ema_model is not None:\n",
    "            torch.save(ema_model.module.state_dict(), f'{OUTPUT_DIR}/best_model_{cfg.seed}.pt')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), f\"{OUTPUT_DIR}/best_model_{cfg.seed}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
